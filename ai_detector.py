"""
AI Content Detector
–î–µ—Ç–µ–∫—Ç–æ—Ä AI-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –≤–∏–¥–µ–æ, –∞—É–¥–∏–æ)
"""

import cv2
import numpy as np
from PIL import Image
import librosa
import io
from typing import Dict, Tuple
import hashlib


class AIContentDetector:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä AI-—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self):
        self.image_features = []
        self.audio_features = []
        
    def detect_image(self, image_path: str) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            img = cv2.imread(image_path)
            if img is None:
                return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ"}
            
            results = {
                "type": "image",
                "ai_probability": 0.0,
                "indicators": [],
                "details": {}
            }
            
            # 1. –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (EXIF)
            metadata_score = self._analyze_metadata(image_path)
            results["details"]["metadata_analysis"] = metadata_score
            
            # 2. –ê–Ω–∞–ª–∏–∑ —à—É–º–∞ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            noise_score = self._analyze_noise_patterns(img)
            results["details"]["noise_patterns"] = noise_score
            
            # 3. –ê–Ω–∞–ª–∏–∑ —Å–∏–º–º–µ—Ç—Ä–∏–∏ –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            symmetry_score = self._analyze_symmetry(img)
            results["details"]["symmetry_analysis"] = symmetry_score
            
            # 4. –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            frequency_score = self._analyze_frequency(img)
            results["details"]["frequency_analysis"] = frequency_score
            
            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–∏–ø–∏—á–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã GAN/Diffusion
            artifact_score = self._detect_ai_artifacts(img)
            results["details"]["ai_artifacts"] = artifact_score
            
            # –†–∞—Å—á–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            weights = {
                "metadata": 0.15,
                "noise": 0.25,
                "symmetry": 0.15,
                "frequency": 0.20,
                "artifacts": 0.25
            }
            
            total_score = (
                metadata_score * weights["metadata"] +
                noise_score * weights["noise"] +
                symmetry_score * weights["symmetry"] +
                frequency_score * weights["frequency"] +
                artifact_score * weights["artifacts"]
            )
            
            results["ai_probability"] = round(total_score * 100, 2)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            if metadata_score > 0.6:
                results["indicators"].append("–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ/–ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")
            if noise_score > 0.7:
                results["indicators"].append("–ù–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —à—É–º–∞")
            if symmetry_score > 0.6:
                results["indicators"].append("–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–∏–º–º–µ—Ç—Ä–∏–∏")
            if frequency_score > 0.7:
                results["indicators"].append("–ê–Ω–æ–º–∞–ª–∏–∏ –≤ —á–∞—Å—Ç–æ—Ç–Ω–æ–º —Å–ø–µ–∫—Ç—Ä–µ")
            if artifact_score > 0.7:
                results["indicators"].append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã AI-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã")
            
            return results
            
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"}
    
    def detect_video(self, video_path: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ"}
            
            results = {
                "type": "video",
                "ai_probability": 0.0,
                "indicators": [],
                "details": {
                    "frames_analyzed": 0,
                    "temporal_consistency": 0.0
                }
            }
            
            frame_scores = []
            frame_count = 0
            max_frames = 30  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä
            
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            step = max(1, total_frames // max_frames)
            
            prev_frame = None
            temporal_scores = []
            
            while cap.isOpened() and frame_count < max_frames:
                ret, frame = cap.read()
                if not ret:
                    break
                
                if frame_count % step == 0:
                    # –ê–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –∫–∞–¥—Ä–∞
                    noise_score = self._analyze_noise_patterns(frame)
                    artifact_score = self._detect_ai_artifacts(frame)
                    frame_score = (noise_score + artifact_score) / 2
                    frame_scores.append(frame_score)
                    
                    # –ê–Ω–∞–ª–∏–∑ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
                    if prev_frame is not None:
                        temporal_score = self._analyze_temporal_consistency(prev_frame, frame)
                        temporal_scores.append(temporal_score)
                    
                    prev_frame = frame.copy()
                
                frame_count += 1
            
            cap.release()
            
            if frame_scores:
                avg_frame_score = np.mean(frame_scores)
                avg_temporal_score = np.mean(temporal_scores) if temporal_scores else 0.5
                
                # AI-–≤–∏–¥–µ–æ —á–∞—Å—Ç–æ –∏–º–µ–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å—é
                results["ai_probability"] = round(
                    (avg_frame_score * 0.6 + avg_temporal_score * 0.4) * 100, 2
                )
                results["details"]["frames_analyzed"] = len(frame_scores)
                results["details"]["temporal_consistency"] = round(avg_temporal_score * 100, 2)
                
                if avg_frame_score > 0.7:
                    results["indicators"].append("AI-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≤ –∫–∞–¥—Ä–∞—Ö")
                if avg_temporal_score > 0.7:
                    results["indicators"].append("–ù–∞—Ä—É—à–µ–Ω–∏–µ —Ç–µ–º–ø–æ—Ä–∞–ª—å–Ω–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏")
            
            return results
            
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ: {str(e)}"}
    
    def detect_audio(self, audio_path: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ AI-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        try:
            y, sr = librosa.load(audio_path, sr=None)
            
            results = {
                "type": "audio",
                "ai_probability": 0.0,
                "indicators": [],
                "details": {}
            }
            
            # 1. –ê–Ω–∞–ª–∏–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
            spectral_score = self._analyze_audio_spectrum(y, sr)
            results["details"]["spectral_analysis"] = spectral_score
            
            # 2. –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            pattern_score = self._analyze_audio_patterns(y, sr)
            results["details"]["pattern_analysis"] = pattern_score
            
            # 3. –ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ (–¥–ª—è –≥–æ–ª–æ—Å–∞)
            naturalness_score = self._analyze_voice_naturalness(y, sr)
            results["details"]["naturalness"] = naturalness_score
            
            # –†–∞—Å—á–µ—Ç –∏—Ç–æ–≥–æ–≤–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            total_score = (
                spectral_score * 0.35 +
                pattern_score * 0.35 +
                naturalness_score * 0.30
            )
            
            results["ai_probability"] = round(total_score * 100, 2)
            
            if spectral_score > 0.7:
                results["indicators"].append("–ê–Ω–æ–º–∞–ª–∏–∏ –≤ —Å–ø–µ–∫—Ç—Ä–µ")
            if pattern_score > 0.7:
                results["indicators"].append("–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
            if naturalness_score > 0.7:
                results["indicators"].append("–ù–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–≤—É—á–∞–Ω–∏–µ")
            
            return results
            
        except Exception as e:
            return {"error": f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∞—É–¥–∏–æ: {str(e)}"}
    
    # === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ===
    
    def _analyze_metadata(self, image_path: str) -> float:
        """–ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        try:
            from PIL import Image
            from PIL.ExifTags import TAGS
            
            img = Image.open(image_path)
            exif = img._getexif()
            
            if exif is None or len(exif) < 3:
                # AI-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–∞—Å—Ç–æ –Ω–µ –∏–º–µ—é—Ç EXIF –¥–∞–Ω–Ω—ã—Ö
                return 0.8
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–∏–ø–∏—á–Ω—ã–µ –ø–æ–ª—è –∫–∞–º–µ—Ä—ã
            camera_fields = ['Make', 'Model', 'DateTime', 'Software']
            has_camera_info = any(
                TAGS.get(tag, tag) in camera_fields 
                for tag in exif.keys()
            )
            
            return 0.2 if has_camera_info else 0.7
            
        except:
            return 0.5
    
    def _analyze_noise_patterns(self, img: np.ndarray) -> float:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —à—É–º–∞"""
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
        
        # –í—ã—Å–æ–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–∞—è —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∞—è (—à—É–º)
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        noise_std = np.std(laplacian)
        
        # AI-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –Ω–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∏–∑–∫–∏–π –∏–ª–∏ –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å —à—É–º–∞
        natural_noise_range = (5, 25)
        
        if noise_std < natural_noise_range[0]:
            return 0.7  # –°–ª–∏—à–∫–æ–º "—á–∏—Å—Ç–æ–µ"
        elif noise_std > natural_noise_range[1] * 2:
            return 0.6  # –°–ª–∏—à–∫–æ–º –∑–∞—à—É–º–ª–µ–Ω–Ω–æ–µ
        else:
            return 0.3
    
    def _analyze_symmetry(self, img: np.ndarray) -> float:
        """–ê–Ω–∞–ª–∏–∑ —Å–∏–º–º–µ—Ç—Ä–∏–∏ –∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ª–µ–≤–æ–π –∏ –ø—Ä–∞–≤–æ–π –ø–æ–ª–æ–≤–∏–Ω
        h, w = gray.shape
        left = gray[:, :w//2]
        right = cv2.flip(gray[:, w//2:], 1)
        
        min_width = min(left.shape[1], right.shape[1])
        left = left[:, :min_width]
        right = right[:, :min_width]
        
        similarity = np.corrcoef(left.flatten(), right.flatten())[0, 1]
        
        # AI —á–∞—Å—Ç–æ —Å–æ–∑–¥–∞–µ—Ç —Å–ª–∏—à–∫–æ–º —Å–∏–º–º–µ—Ç—Ä–∏—á–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if similarity > 0.85:
            return 0.7
        else:
            return 0.3
    
    def _analyze_frequency(self, img: np.ndarray) -> float:
        """–ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —á–µ—Ä–µ–∑ FFT"""
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
        
        # FFT
        f_transform = np.fft.fft2(gray)
        f_shift = np.fft.fftshift(f_transform)
        magnitude = np.abs(f_shift)
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç
        h, w = magnitude.shape
        center_region = magnitude[h//4:3*h//4, w//4:3*w//4]
        edge_region = magnitude.copy()
        edge_region[h//4:3*h//4, w//4:3*w//4] = 0
        
        center_energy = np.sum(center_region)
        edge_energy = np.sum(edge_region)
        
        ratio = center_energy / (edge_energy + 1)
        
        # AI-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –∞–Ω–æ–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç
        if ratio > 100 or ratio < 10:
            return 0.7
        else:
            return 0.3
    
    def _detect_ai_artifacts(self, img: np.ndarray) -> float:
        """–î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–∏—á–Ω—ã—Ö AI-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
        score = 0.0
        count = 0
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ "checkerboard artifacts" (—à–∞—Ö–º–∞—Ç–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤—Ç–æ—Ä–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        checker_variance = np.var(laplacian)
        
        if checker_variance > 1000:
            score += 0.8
        count += 1
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–Ω–æ—Å—Ç–∏ –≤ –∫—Ä–∞—è—Ö –æ–±—ä–µ–∫—Ç–æ–≤
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        if edge_density < 0.05 or edge_density > 0.3:
            score += 0.6
        count += 1
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–≤–µ—Ç–æ–≤—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π
        if len(img.shape) == 3:
            hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
            saturation = hsv[:, :, 1]
            
            # AI —á–∞—Å—Ç–æ —Å–æ–∑–¥–∞–µ—Ç –Ω–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞—Å—ã—â–µ–Ω–Ω—ã–µ —Ü–≤–µ—Ç–∞
            if np.mean(saturation) > 180:
                score += 0.7
            count += 1
        
        return score / count if count > 0 else 0.5
    
    def _analyze_temporal_consistency(self, frame1: np.ndarray, frame2: np.ndarray) -> float:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É –∫–∞–¥—Ä–∞–º–∏"""
        # –û–ø—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ç–æ–∫
        gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
        
        flow = cv2.calcOpticalFlowFarneback(
            gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0
        )
        
        # –ê–Ω–∞–ª–∏–∑ –≤–µ–ª–∏—á–∏–Ω—ã –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø–æ—Ç–æ–∫–∞
        magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)
        
        # AI-–≤–∏–¥–µ–æ —á–∞—Å—Ç–æ –∏–º–µ–µ—Ç —Ä–µ–∑–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–ª–∏ –Ω–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø–ª–∞–≤–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
        mean_flow = np.mean(magnitude)
        std_flow = np.std(magnitude)
        
        if mean_flow > 20 or (mean_flow < 2 and std_flow < 1):
            return 0.7
        else:
            return 0.3
    
    # === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞—É–¥–∏–æ ===
    
    def _analyze_audio_spectrum(self, y: np.ndarray, sr: int) -> float:
        """–ê–Ω–∞–ª–∏–∑ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
        # –°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞
        spec = np.abs(librosa.stft(y))
        
        # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥
        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]
        
        # AI-–≥–æ–ª–æ—Å —á–∞—Å—Ç–æ –∏–º–µ–µ—Ç –Ω–µ–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç
        mean_centroid = np.mean(spectral_centroids)
        
        if mean_centroid < 1000 or mean_centroid > 4000:
            return 0.7
        else:
            return 0.3
    
    def _analyze_audio_patterns(self, y: np.ndarray, sr: int) -> float:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –∞—É–¥–∏–æ"""
        # Zero crossing rate
        zcr = librosa.feature.zero_crossing_rate(y)[0]
        
        # AI-–∞—É–¥–∏–æ —á–∞—Å—Ç–æ –∏–º–µ–µ—Ç —Å–ª–∏—à–∫–æ–º —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        zcr_std = np.std(zcr)
        
        if zcr_std < 0.01:
            return 0.7
        else:
            return 0.3
    
    def _analyze_voice_naturalness(self, y: np.ndarray, sr: int) -> float:
        """–ê–Ω–∞–ª–∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –≥–æ–ª–æ—Å–∞"""
        # MFCC (Mel-frequency cepstral coefficients)
        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
        
        # –ê–Ω–∞–ª–∏–∑ –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
        mfcc_var = np.var(mfccs, axis=1)
        
        # –°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥–æ–ª–æ—Å —á–∞—Å—Ç–æ –∏–º–µ–µ—Ç –º–µ–Ω—å—à—É—é –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        mean_var = np.mean(mfcc_var)
        
        if mean_var < 10:
            return 0.7
        else:
            return 0.3


def format_result(result: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    if "error" in result:
        return f"‚ùå {result['error']}"
    
    ai_prob = result["ai_probability"]
    content_type = result["type"]
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–¥–∏–∫—Ç–∞
    if ai_prob >= 70:
        verdict = "ü§ñ –í–µ—Ä–æ—è—Ç–Ω–æ —Å–æ–∑–¥–∞–Ω–æ AI"
        emoji = "üî¥"
    elif ai_prob >= 40:
        verdict = "‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ —Å–æ–∑–¥–∞–Ω–æ AI"
        emoji = "üü°"
    else:
        verdict = "‚úÖ –í–µ—Ä–æ—è—Ç–Ω–æ —Å–æ–∑–¥–∞–Ω–æ —á–µ–ª–æ–≤–µ–∫–æ–º"
        emoji = "üü¢"
    
    output = f"{emoji} {verdict}\n"
    output += f"üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å AI: {ai_prob}%\n\n"
    
    if result["indicators"]:
        output += "üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\n"
        for indicator in result["indicators"]:
            output += f"  ‚Ä¢ {indicator}\n"
        output += "\n"
    
    output += "üìà –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:\n"
    for key, value in result["details"].items():
        key_ru = {
            "metadata_analysis": "–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ",
            "noise_patterns": "–ü–∞—Ç—Ç–µ—Ä–Ω—ã —à—É–º–∞",
            "symmetry_analysis": "–°–∏–º–º–µ—Ç—Ä–∏—è",
            "frequency_analysis": "–ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
            "ai_artifacts": "AI-–∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã",
            "frames_analyzed": "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∫–∞–¥—Ä–æ–≤",
            "temporal_consistency": "–¢–µ–º–ø–æ—Ä–∞–ª—å–Ω–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å",
            "spectral_analysis": "–°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑",
            "pattern_analysis": "–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤",
            "naturalness": "–ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"
        }.get(key, key)
        
        if isinstance(value, float):
            output += f"  ‚Ä¢ {key_ru}: {round(value * 100, 1)}%\n"
        else:
            output += f"  ‚Ä¢ {key_ru}: {value}\n"
    
    return output


if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    detector = AIContentDetector()
    print("AI Content Detector –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
